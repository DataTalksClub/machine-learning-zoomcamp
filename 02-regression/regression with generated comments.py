# ğŸš— CAR PRICE PREDICTION MODEL â€” EXPLAINED LIKE YOU'RE 10 ğŸ­
# Every comment tells you: WHAT, WHY, WHAT IF YOU SKIP, and WHAT THE RESULT MEANS!

# Import necessary libraries â€” our toolbox!
import pandas as pd          # Like a super notebook for data â€” rows and columns!
import numpy as np           # Math helper â€” does fast calculations on big lists of numbers
import matplotlib.pyplot as plt  # Drawing tool â€” makes graphs and charts
import seaborn as sns        # Fancy drawing tool â€” makes prettier graphs
from sklearn.linear_model import LinearRegression  # Smart robot that learns patterns from numbers
from sklearn.metrics import mean_squared_error     # Robotâ€™s report card â€” tells us how wrong it was
import warnings              # Hides scary-looking (but harmless) messages
warnings.filterwarnings('ignore')

# Set matplotlib style for better visualizations â€” makes graphs cute and clean!
plt.style.use('seaborn-v0_8')

# ğŸ“‚ STEP 1: LOAD THE DATA â€” Open the car notebook!
print("Loading car data...")
df = pd.read_csv('car_data.csv')  # Load data from file â€” like opening your toy box
print(f"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns")
# ğŸ§’ WHY? We need data to teach the robot! Each row = one car, each column = a fact (price, color, etc.)
# âš ï¸ SKIP? No data = no learning. Robot sits idle.
# ğŸ“Š RESULT: If it says 10,000 rows â€” thatâ€™s 10,000 cars to learn from!

# ğŸ‘€ STEP 2: LOOK AT THE DATA â€” Peek inside the toy box!
print("\nFirst 5 rows of the dataset:")
print(df.head())  # Show first 5 cars â€” like flipping first pages of a book

print("\nDataset info:")
print(df.info())  # Tells us which columns are text/numbers, and if any are empty

print("\nDescriptive statistics:")
print(df.describe())  # Gives averages, min/max â€” e.g., â€œaverage car price is $30,000â€

print("\nMissing values per column:")
print(df.isnull().sum())  # Counts holes in data â€” like missing LEGO pieces
# ğŸ§’ WHY? Robot gets confused by missing or weird data. We check for traps!
# âš ï¸ SKIP? Robot might learn nonsense â€” e.g., â€œif price is missing, guess $0!â€
# ğŸ“Š RESULT: Big number in â€œengine_hpâ€? Weâ€™ll need to fix that later.

# ğŸ•µï¸ STEP 3: VISUALIZE MISSING VALUES â€” Draw the holes!
try:
    import missingno as msno
    print("\nVisualizing missing values...")

    msno.matrix(df)  # Shows missing data as white gaps â€” like holes in Swiss cheese!
    plt.title("Missing Values Matrix")
    plt.show()

    msno.bar(df)     # Bar chart â€” taller bar = more complete data
    plt.title("Missing Values Bar Chart")
    plt.show()

    msno.heatmap(df) # Shows if missingness in one column relates to another
    plt.title("Missingness Correlation Heatmap")
    plt.show()

    msno.dendrogram(df) # Groups columns by how similarly theyâ€™re missing
    plt.title("Missingness Dendrogram")
    plt.show()
except ImportError:
    print("Missingno library not available. Skipping missing value visualizations.")
# ğŸ§’ WHY? Pictures help us see patterns â€” maybe all electric cars are missing â€œgas mileageâ€?
# âš ï¸ SKIP? We might miss BIG problems â€” like â€œconvertibleâ€ cars missing â€œroof typeâ€!
# ğŸ“Š RESULT: White gaps = missing data. Clusters = related missingness â€” fix together!

# ğŸ§© STEP 4: SPLIT DATA â€” Homework, Quiz, Final Exam!
print("\nSetting up validation framework...")
n = len(df)
n_val = int(0.2 * n)   # 20% â†’ validation (quiz)
n_test = int(0.2 * n)  # 20% â†’ test (final exam)
n_train = n - n_val - n_test  # 60% â†’ training (homework)

print(f"Training set size: {n_train}")
print(f"Validation set size: {n_val}")
print(f"Test set size: {n_test}")

# Shuffle data â€” mix up the cars so robot doesnâ€™t cheat!
print("\nShuffling data...")
np.random.seed(2)  # Same shuffle every time â€” like using same dice
idx = np.arange(n)
np.random.shuffle(idx)

df_shuffled = df.iloc[idx]
df_val = df_shuffled.iloc[:n_val]         # First 20% â†’ quiz
df_test = df_shuffled.iloc[n_val:n_val+n_test]  # Next 20% â†’ final exam
df_train = df_shuffled.iloc[n_val+n_test:]      # Last 60% â†’ homework

# Reset index so no confusion later
df_train = df_train.reset_index(drop=True)
df_val = df_val.reset_index(drop=True)
df_test = df_test.reset_index(drop=True)

print("Data split completed.")
# ğŸ§’ WHY? Robot must learn on homework, get graded on quiz, and take secret final exam!
# âš ï¸ SKIP? Robot memorizes answers â†’ fails in real world. Like cheating on test!
# ğŸ“Š RESULT: Train=learn, Val=tweak, Test=TRUE score. Never touch Test until the end!

# ğŸ¨ STEP 5: CLEAN TEXT â€” Make it robot-friendly!
print("Cleaning string columns...")
string_columns = df.select_dtypes(include=['object']).columns
for col in string_columns:
    # Turn "Red Car" â†’ "red_car" so robot doesnâ€™t think â€œRedâ€ â‰  â€œredâ€
    df[col] = df[col].str.lower().str.replace(" ", "_")
    df_train[col] = df_train[col].str.lower().str.replace(" ", "_")
    df_val[col] = df_val[col].str.lower().str.replace(" ", "_")
    df_test[col] = df_test[col].str.lower().str.replace(" ", "_")

print("String columns cleaned.")
# ğŸ§’ WHY? Robot thinks â€œTOYOTAâ€, â€œToyotaâ€, â€œtoyotaâ€ are 3 different brands! We make them same.
# âš ï¸ SKIP? Robot splits data wrong â†’ bad learning. â€œtoyotaâ€ gets ignored, â€œTOYOTAâ€ gets all attention.
# ğŸ“Š RESULT: All text is lowercase with underscores â€” clean and consistent!

# ğŸ“ˆ STEP 6: LOOK AT PRICE â€” Is it fair or crazy?
print("\nVisualizing price distribution...")
plt.figure(figsize=(10, 6))
sns.histplot(df.msrp[df.msrp < 100000], bins=50)  # Ignore super expensive cars for now
plt.title('Distribution of Car Prices (MSRP)')
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.show()
# ğŸ§’ WHY? Most cars are $20Kâ€“$50K, but Ferraris are $500K! Robot gets distracted by rare cars.
# âš ï¸ SKIP? Robot cares too much about Lambos â†’ fails on normal cars.
# ğŸ“Š RESULT: Long tail to the right = skewed. Needs fixing!

# ğŸ“ STEP 7: LOG-TRANSFORM PRICE â€” Squish the rich cars!
print("\nApplying log1p transformation to price...")
price_log = np.log1p(df.msrp)  # log1p(x) = log(1+x) â€” safe even if price=0!
print("First 5 log-transformed prices:")
print(price_log.head())

plt.figure(figsize=(10, 6))
sns.histplot(price_log, bins=50)
plt.title('Distribution of Log-Transformed Car Prices')
plt.xlabel('Log(1 + Price)')
plt.ylabel('Count')
plt.show()
# ğŸ§’ WHY? Turns $1, $10, $100, $1000 â†’ 0, 1, 2, 3. Squishes rich cars next to normal ones!
# âš ï¸ SKIP? Robot obsessed with outliers â†’ bad predictions for regular cars.
# ğŸ“Š RESULT: Bell curve! Robotâ€™s favorite shape. Errors now balanced.

# ğŸ¯ STEP 8: PREPARE TARGET (Y) â€” What we want to predict!
print("\nPreparing target variables...")
y_train = np.log1p(df_train.msrp.values)  # Log-price for training cars
y_val = np.log1p(df_val.msrp.values)
y_test = np.log1p(df_test.msrp.values)

# REMOVE PRICE FROM FEATURES â€” no peeking at answers!
del df_train['msrp']
del df_val['msrp']
del df_test['msrp']

print("Target variables prepared.")
# ğŸ§’ WHY? Robot must guess price â€” not copy it! Like hiding answer key during test.
# âš ï¸ SKIP? Robot sees price â†’ 100% accurate but USELESS. Like cheating.
# ğŸ“Š RESULT: y_train = list of correct answers robot must learn to guess.

# ğŸ§± STEP 9: PICK FEATURES â€” What clues help guess price?
base_features = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']
print(f"\nBase features: {base_features}")

categorical_vars = ['make', 'engine_fuel_type', 'transmission_type', 'driven_wheels', 
                   'market_category', 'vehicle_size', 'vehicle_style']
print(f"Categorical variables: {categorical_vars}")

# Get top 5 most common categories â€” avoid rare ones that confuse robot!
print("\nIdentifying most frequent categories...")
categories = {}
for var in categorical_vars:
    categories[var] = list(df[var].value_counts().head().index)
    print(f"{var}: {categories[var]}")

popular_makes = list(df.make.value_counts().head().index)
print(f"Most popular makes: {popular_makes}")
# ğŸ§’ WHY? Robot only understands numbers. â€œToyotaâ€ â†’ becomes a 1/0 switch. Only top 5 to avoid clutter.
# âš ï¸ SKIP? Too many categories â†’ robot overfits. â€œCars with â€˜xâ€™ in name cost more?!â€ â€” nonsense!
# ğŸ“Š RESULT: â€œmake_toyotaâ€ = 1 if Toyota, 0 otherwise. Robot learns brand effect.

# ğŸ§® STEP 10: PREPARE FEATURES (X) â€” Build robotâ€™s LEGO set!
def prepare_X(df_input):
    """
    ğŸ§’ WHY? Robot needs clean numbered blocks to play with.
    We add age, door switches, brand switches, fill holes with 0.
    âš ï¸ SKIP? Robot crashes on missing values or text.
    ğŸ“Š RESULT: X = big table of numbers â€” robot food!
    """
    df_copy = df_input.copy()
    features = base_features.copy()  # Start with engine, mpg, etc.

    # Add car age â€” older cars usually cheaper!
    df_copy['age'] = 2017 - df_copy.year
    features.append('age')

    # Add door switches â€” is it 2-door? 4-door?
    for num_doors in [2, 3, 4]:
        df_copy[f'num_doors_{num_doors}'] = (df_copy.number_of_doors == num_doors).astype('int')
        features.append(f'num_doors_{num_doors}')

    # Add brand switches â€” is it Toyota? Ford?
    for make in popular_makes:
        df_copy[f'make_{make}'] = (df_copy.make == make).astype('int')
        features.append(f'make_{make}')

    # Add category switches â€” automatic? AWD? SUV?
    for cat_var, cat_values in categories.items():
        for value in cat_values:
            df_copy[f'{cat_var}_{value}'] = (df_copy[cat_var] == value).astype('int')
            features.append(f'{cat_var}_{value}')

    # Keep only prepared features
    df_numeric = df_copy[features]

    # Fill holes with 0 â€” â€œif we donâ€™t know, assume averageâ€
    df_numeric = df_numeric.fillna(0)

    # Give robot the LEGO set!
    X = df_numeric.values
    return X

# ğŸ¤– STEP 11: TRAIN ROBOT â€” Find best guessing rules!
def train_linear_regression(X, y):
    """
    ğŸ§’ WHY? Robot learns: price = w0 + w1*HP + w2*is_Toyota + ...
    We calculate best wâ€™s using math magic (matrix inversion).
    âš ï¸ SKIP? No model = no predictions. Just guessing!
    ğŸ“Š RESULT: w0 = base price. w[0] = how much HP adds to price.
    """
    ones = np.ones(X.shape[0])
    X = np.column_stack([ones, X])  # Add column for w0 (intercept)

    XTX = X.T.dot(X)        # Robotâ€™s â€œmemory matrixâ€
    XTX_inv = np.linalg.inv(XTX)  # Solve the puzzle!
    w_full = XTX_inv.dot(X.T).dot(y)  # Best weights ever!

    w0 = w_full[0]  # Base price (if all features=0)
    w = w_full[1:]  # How much each feature changes price
    return w0, w

# ğŸ“ STEP 12: SCORE ROBOT â€” How wrong is it?
def rmse(y_true, y_pred):
    """
    ğŸ§’ WHY? Robotâ€™s report card. Lower = better.
    Measures average mistake in log-price units.
    âš ï¸ SKIP? We wouldnâ€™t know if robot is good or bad.
    ğŸ“Š RESULT: RMSE=0.3 â†’ e^0.3â‰ˆ1.35 â†’ robot off by ~35% in real dollars.
    """
    error = y_true - y_pred
    se = error ** 2
    mse = se.mean()
    return np.sqrt(mse)

# ğŸ¯ STEP 13: TRAIN & SCORE BASE MODEL â€” Homework grade!
print("\nTraining linear regression model...")
X_train = prepare_X(df_train)
w0, w = train_linear_regression(X_train, y_train)

X_val = prepare_X(df_val)
y_pred = w0 + X_val.dot(w)

val_rmse = rmse(y_val, y_pred)
print(f"Validation RMSE: {val_rmse:.6f}")
# ğŸ§’ WHY? Teach on homework, grade on quiz.
# âš ï¸ SKIP? No idea if robot learned anything.
# ğŸ“Š RESULT: ~0.5 = okay, ~0.2 = great! Weâ€™ll try to improve.

# ğŸ¨ STEP 14: PLOT PREDICTIONS â€” Are dots near the line?
plt.figure(figsize=(10, 6))
plt.scatter(y_val, y_pred, alpha=0.5)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
plt.xlabel('Actual Log Price')
plt.ylabel('Predicted Log Price')
plt.title('Predictions vs Actual Values (Validation Set)')
plt.show()
# ğŸ§’ WHY? Picture > numbers. Dots on line = good! Cloud = bad.
# âš ï¸ SKIP? Might miss bias â€” robot always guesses low.
# ğŸ“Š RESULT: Look for patterns â€” hard to predict luxury cars?

# ğŸ”„ STEP 15: ADD TRAINING WHEELS â€” Regularization!
def train_linear_regression_reg(X, y, r=0.001):
    """
    ğŸ§’ WHY? Sometimes robot memorizes noise (â€œcars with â€˜zâ€™ cost more!â€).
    Regularization says: â€œDonâ€™t trust any one feature too much!â€
    âš ï¸ SKIP? Robot overfits â€” perfect on homework, fails on quiz.
    ğŸ“Š RESULT: Small r = gentle. Big r = robot ignores features. Find sweet spot!
    """
    ones = np.ones(X.shape[0])
    X = np.column_stack([ones, X])

    XTX = X.T.dot(X)
    reg = r * np.eye(XTX.shape[0])  # Penalty matrix
    reg[0, 0] = 0  # Donâ€™t penalize base price

    XTX = XTX + reg  # Add penalty
    XTX_inv = np.linalg.inv(XTX)
    w_full = XTX_inv.dot(X.T).dot(y)

    w0 = w_full[0]
    w = w_full[1:]
    return w0, w

# ğŸ” STEP 16: FIND BEST TRAINING WHEELS â€” Tune r!
print("\nFinding optimal regularization parameter...")
r_values = [0, 0.000001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10]
rmse_scores = []

for r in r_values:
    X_train_reg = prepare_X(df_train)
    w0_reg, w_reg = train_linear_regression_reg(X_train_reg, y_train, r=r)

    X_val_reg = prepare_X(df_val)
    y_pred_reg = w0_reg + X_val_reg.dot(w_reg)

    score = rmse(y_val, y_pred_reg)
    rmse_scores.append(score)
    print(f"r={r}: RMSE={score:.6f}")

# Plot RMSE vs r â€” find the valley!
plt.figure(figsize=(10, 6))
plt.plot(r_values, rmse_scores, 'bo-')
plt.xscale('log')
plt.xlabel('Regularization Parameter (r)')
plt.ylabel('RMSE')
plt.title('RMSE vs Regularization Parameter')
plt.grid(True, alpha=0.3)
plt.show()
# ğŸ§’ WHY? Test different tightness of training wheels.
# âš ï¸ SKIP? Might use too much or too little â†’ robot underfits or overfits.
# ğŸ“Š RESULT: Lowest point = best r. Usually 0.001â€“0.1.

# ğŸ STEP 17: FINAL MODEL & TEST SET â€” FINAL EXAM!
best_idx = np.argmin(rmse_scores)
best_r = r_values[best_idx]
best_rmse = rmse_scores[best_idx]

print(f"\nBest regularization parameter: r={best_r}")
print(f"Best RMSE: {best_rmse:.6f}")

print("\nTraining final model with optimal regularization...")
X_train_final = prepare_X(df_train)
w0_final, w_final = train_linear_regression_reg(X_train_final, y_train, r=best_r)

X_val_final = prepare_X(df_val)
y_pred_final = w0_final + X_val_final.dot(w_final)
final_rmse = rmse(y_val, y_pred_final)
print(f"Final model RMSE (validation): {final_rmse:.6f}")

# ğŸ§ª TEST ON SECRET TEST SET â€” TRUE SCORE!
print("\nEvaluating final model on test set...")
X_test = prepare_X(df_test)
y_pred_test = w0_final + X_test.dot(w_final)
test_rmse = rmse(y_test, y_pred_test)
print(f"TEST RMSE (final exam!): {test_rmse:.6f}")
# ğŸ§’ WHY? Final exam â€” never touched during training. TRUE measure of skill.
# âš ï¸ SKIP? Weâ€™d be lying to ourselves. Like grading your own test.
# ğŸ“Š RESULT: Should be close to validation RMSE. Much worse? Overfit. Better? Lucky (or bug ğŸ˜‰).

# ğŸ’° STEP 18: PREDICT NEW CARS â€” Magic 8-ball for prices!
def predict_car_price(car_features, w0=w0_final, w=w_final):
    """
    ğŸ§’ WHY? Now YOU can ask: â€œHow much is this car?â€
    âš ï¸ SKIP? All that work for nothing â€” no real use!
    ğŸ“Š RESULT: Robot says $28,500? Check real listings. Close? Robot is smart!
    """
    if isinstance(car_features, dict):
        car_df = pd.DataFrame([car_features])
    else:
        car_df = car_features.copy()

    X_car = prepare_X(car_df)           # Build LEGO set for this car
    y_pred_log = w0 + X_car.dot(w)      # Robotâ€™s guess (log scale)
    predicted_price = np.expm1(y_pred_log)  # Convert back to real dollars!

    return predicted_price

# ğŸš— EXAMPLE: Predict price for a specific car!
print("\nExample prediction for a specific car:")
example_car = {
    'make': 'chevrolet',
    'model': 'trailblazer_ext',
    'year': 2004,
    'engine_fuel_type': 'regular_unleaded',
    'engine_hp': 275.0,
    'engine_cylinders': 6.0,
    'transmission_type': 'automatic',
    'driven_wheels': 'rear_wheel_drive',
    'number_of_doors': 4.0,
    'market_category': 'crossover',
    'vehicle_size': 'large',
    'vehicle_style': '4dr_suv',
    'highway_mpg': 18,
    'city_mpg': 13,
    'popularity': 1385
}

predicted_price = predict_car_price(example_car)
print(f"Predicted price: ${predicted_price:,.0f}")

# âœ… CONGRATS! You built a car price wizard! ğŸ§™â€â™‚ï¸ğŸš—ğŸ’¸
# Remember:
# - Clean data â†’ happy robot
# - Split data â†’ honest robot
# - Regularize â†’ wise robot
# - Test set â†’ true robot score
# Now go predict some cars!